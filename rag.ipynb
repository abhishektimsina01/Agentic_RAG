{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "bb06bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import interrupt, Command\n",
    "from typing import TypedDict, Annotated, Literal, Optional, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import operator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "acc39269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the models\n",
    "load_dotenv()\n",
    "model = ChatGroq(\n",
    "    # model=os.getenv(\"model\")\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")\n",
    "\n",
    "evaluator = ChatGroq(\n",
    "    model=os.getenv(\"evaluater_model\"),\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "4f9875a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path=\"Attention.pdf\")\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "7196cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 80)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "e5682118",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    embedding=embedding_model,\n",
    "    documents=chunks,\n",
    "    collection_name= \"my_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "4e21371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I am using mmr so that each chunk retrieved covers different angles\n",
    "retrievers = vector_store.as_retriever(search_type = \"mmr\", search_kwargs = {'k' : 2, 'fech_k' : 5, 'lambda_mult' : 0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "1e841bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Format(BaseModel):\n",
    "    output :list[str]\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    evaluation : float = Field(ge=0.0, le=1.0, description=\"this containt the value got for the answer providd to the llm after evaluation.\")\n",
    "    feedback : str = Field(description = \"this should containt the place where the refining should be done, it is like feedback to imporive the evaluation\")\n",
    "\n",
    "class Refinied(BaseModel):\n",
    "    latest_question : str = Field(description= \"this should contain the main questions that the other questions are made from\")\n",
    "    sub_questions : list[str] = Field(description= \"this should contain all the questions made from decomposing the main question\")\n",
    "\n",
    "op_format = PydanticOutputParser(pydantic_object=Format)\n",
    "evaluation_format = PydanticOutputParser(pydantic_object=Evaluation)\n",
    "Refine_format = PydanticOutputParser(pydantic_object=Refinied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "13c6e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_state(TypedDict):\n",
    "    latest_question : Annotated[list[BaseMessage], operator.add]\n",
    "    sub_questions : Annotated[list[list[str]], operator.add]\n",
    "    answers : Annotated[list[list[list[str]]], operator.add]\n",
    "    final_answer : str\n",
    "    retry_count : int\n",
    "    evaluation : float\n",
    "    feedback : str\n",
    "    clarification : str\n",
    "    verify : str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "84b47654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I need sqlite db to store the checkpoints to make my agent remember the past topics\n",
    "# conn = sqlite3.connect(database='RAG.db', check_same_thread=False)\n",
    "# # this creates the checkpointer that saves the checkpoint in the database and allows persistance\n",
    "# checkpointer = SqliteSaver(conn=conn)\n",
    "\n",
    "# config = {'configurable' : {'thread_id': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc536c",
   "metadata": {},
   "source": [
    "### Query Decomposer:\n",
    "- Query decomposer is used to avoid getting only the content that matches to the question and focusing on getting all the context from the document from all angles. \n",
    "\n",
    " for example:\n",
    "\n",
    " If the question is **Why shouldn't we smoke?**, then the decomposed questions can be\n",
    " * Why somking is bad?\n",
    " * Somking effect on our health\n",
    " * How does it effect our body?\n",
    "\n",
    "are the questions that can hit different angle, which produces the quality answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "dbc1a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all node's task\n",
    "def query_decomposition(state : RAG_state):\n",
    "    # first when we get the prompt, first we decompose the main question so that we can make multiple diferent questions in the same sense to extract the relevant contnet from the RAG\n",
    "    template = PromptTemplate(template=\"\"\"You are a query decomposition agent.\n",
    "    Your task is to transform a single high-level user question into 2–3 concise, non-overlapping sub-questions optimized for semantic search and retrieval.\n",
    "    Rules:\n",
    "    - Each sub-question must target a distinct angle of the original question.\n",
    "    - Sub-questions must be factual, concrete, and searchable.\n",
    "    - Avoid rephrasing the same question multiple times.\n",
    "    - Do NOT answer the questions.\n",
    "    - Do NOT add explanations.\n",
    "    - Keep each sub-question under 15 words only.\n",
    "    - if the question is simple then make the sub-questions simple too.\n",
    "    - Prefer \"what / how / why\" formulations when useful.\n",
    "    Question: \n",
    "    {question}\n",
    "    Output format:\n",
    "    {fm}\"\"\", input_variables=['question'],partial_variables={'fm' : op_format.get_format_instructions()})\n",
    "    chain = template | model | op_format\n",
    "    output = chain.invoke({\"question\" : state['latest_question'][-1]}).output\n",
    "    print(\"Query Decomposition executed✅\")\n",
    "    return {'sub_questions' : [output]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "8127d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Decomposition executed✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sub_questions': [['What harms smoking cause?',\n",
       "   'How smoking affects health?',\n",
       "   'Why quit smoking?']]}"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = [\"Why is smoking bad?\"])\n",
    "query_decomposition(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "464c00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreiving the relevant content from the docs\n",
    "def retriever(state : RAG_state):\n",
    "    answers = []\n",
    "    # Answer for the main question\n",
    "    answer = retrievers.invoke(state['latest_question'][-1])\n",
    "    answers.append(answer)\n",
    "    # Answers for the sub-question\n",
    "    for question in state['sub_questions'][-1]:\n",
    "        answer = retrievers.invoke(question)\n",
    "        answers.append(answer)\n",
    "    page_content = []\n",
    "    for ans in answers:\n",
    "        element = []\n",
    "        for i in ans:\n",
    "            element.append(i.page_content)\n",
    "        page_content.append(element)\n",
    "    batch_answers = [page_content]\n",
    "    print(\"Retriever executed✅\")\n",
    "    return {'answers' : batch_answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "068b45e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever executed✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answers': [[['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
       "    'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.'],\n",
       "   ['the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.',\n",
       "    'were chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].'],\n",
       "   ['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
       "    'the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.'],\n",
       "   ['the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.',\n",
       "    'keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.']]]}"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = [\"Why is smoking bad?\"], sub_questions=[['What health risks are associated with smoking?',\n",
    "   'How does smoking affect the cardiovascular system?',\n",
    "   'Why does smoking lead to addiction?']])\n",
    "retriever(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "e3d138b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize the answers that we got from each sub-questions in such a way that it provides a different angle for summarizing for each list of sub-question\n",
    "# def summarizer(state : RAG_state):\n",
    "#     answers = state['answers'][-1]\n",
    "#     question = state['latest_question'][-1]\n",
    "#     sub_questions = state['sub_questions'][-1]\n",
    "#     ques_ans_dict : dict[str, list[str]] = {}\n",
    "#     ques_ans_dict[question] = answers[0]\n",
    "#     for index, answer in enumerate(answers):\n",
    "#         if index == 0:\n",
    "#             continue\n",
    "#         ques_ans_dict[sub_questions[index-1]] = answer\n",
    "#     ques_ans_dict_str = json.dumps(ques_ans_dict, indent=2)\n",
    "#     print(type(ques_ans_dict_str))\n",
    "#     template = PromptTemplate(template=\"\"\"You are a summarization agent.\n",
    "#     Task:\n",
    "#     - Summarize the retrieved chunks for each sub-question.\n",
    "#     - Each sub-question maps to a list of concise summary sentences.\n",
    "#     - Summaries may mix information from multiple chunks, but must reflect the original content accurately.\n",
    "#     - Do not hallucinate; use only the content from the chunks.\n",
    "#     - Do not dilute important details.       \n",
    "#     - If there is no context in the answer list based on the question then just say \"No context match\". \n",
    "                                                                                                              \n",
    "#     Input:\n",
    "#     - questions_and_dict: a JSON object where each key is a sub-question, and each value is a list of strings (chunks retrieved for that sub-question).\\n\n",
    "#     {ques_ans_dict_str}\n",
    "                                                \n",
    "#     Output format:\n",
    "#     The output should only contain the answer only, pure string\n",
    "#     {format_ins}\n",
    "#     \"\"\", input_variables = ['ques_ans_dict_str'], partial_variables={'format_ins' : summary_format.get_format_instructions()})\n",
    "#     chain = template | model | summary_format\n",
    "#     result = chain.invoke({\"ques_ans_dict_str\" : ques_ans_dict_str}).summary\n",
    "#     print(\"Summarizer executed✅\")\n",
    "#     print(result)\n",
    "#     return {'final_answer' : result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "f781ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def summarizer(state: RAG_state):\n",
    "    latest_question = state['latest_question'][-1]\n",
    "    sub_questions = state['sub_questions'][-1]\n",
    "    all_answers = state['answers'][-1]  # list of lists of strings\n",
    "\n",
    "    # Build dict mapping question -> list of answer chunks\n",
    "    ques_ans_dict: dict[str, list[str]] = {}\n",
    "    for i, q in enumerate([latest_question] + sub_questions):\n",
    "        if i < len(all_answers):\n",
    "            ques_ans_dict[q] = all_answers[i]\n",
    "        else:\n",
    "            ques_ans_dict[q] = [\"No context match\"]\n",
    "\n",
    "    # Serialize dict to JSON string for the prompt\n",
    "    ques_ans_dict_str = json.dumps(ques_ans_dict, indent=2)\n",
    "    template = PromptTemplate(\n",
    "        template=\"\"\"You are an agent that provides me answer from the sub_question and chunks provided to you.\n",
    "Rules:\n",
    "- Summarize the retrieved chunks for each sub-question.\n",
    "- Summaries may mix information from multiple chunks, but must reflect the original content accurately.\n",
    "- Do not hallucinate; use only the content from the chunks.\n",
    "- Do not dilute important details.\n",
    "- Give me the answer in aggregate not for each sub-question\n",
    "Summarize on the basis of the given information: \\n{ques_ans_dict}\n",
    "\n",
    "**If there is no context in the answer list based on the question then just say \"No context match\" and nothing else**\n",
    "\"\"\",\n",
    "        input_variables=['ques_ans_dict'])\n",
    "\n",
    "    chain = template | model | StrOutputParser()\n",
    "    result = chain.invoke({\"ques_ans_dict\": ques_ans_dict_str})\n",
    "    print(\"Summarizer executed ✅\")\n",
    "    return {'final_answer': result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "2d84293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizer executed ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_answer': 'No context match'}"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = [\"Why is smoking bad?\"], \n",
    "                  sub_questions=[['What health risks are associated with smoking?', 'How does smoking affect the cardiovascular system?','Why does smoking lead to addiction?']], \n",
    "                  answers=[[['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
    "                            'heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training'],\n",
    "                            ['the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.',\n",
    "                            'were chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].'],\n",
    "                            ['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
    "                            'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture'],\n",
    "                            ['the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.',\n",
    "                            'nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.']]])\n",
    "summarizer(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "9d668ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the asnwer based on the question and sub_questions generated, if the evaluation is > 0.7 then pass and if still better can be done then retry after refining the query and clarification\n",
    "def evaluation(state : RAG_state):\n",
    "    latest_question = state['latest_question'][-1]\n",
    "    questions = state['sub_questions'][-1]\n",
    "    final_answer = state['final_answer']\n",
    "    prompt = PromptTemplate(template=\"\"\"You are an evaluation agent.\n",
    "    Task:\n",
    "    - Evaluate the quality of the provided answer to the given question.\n",
    "    - Provide a **score between 0 and 1**, where 1 is perfect and 0 is completely wrong.\n",
    "    - Provide **feedback** explaining why the score was given and suggestions to improve the answer (e.g., missing information, inaccurate, vague, unclear wording).\n",
    "    - Be concise but informative in the feedback.\n",
    "    - Base your evaluation **strictly on the question and the provided answer**. Do not assume extra knowledge.\n",
    "    - Do NOT include the question in the output, only the score and feedback.\n",
    "\n",
    "    Input:\n",
    "    - question: the original user question\n",
    "    {latest_question} \\n\n",
    "    {sub_questions}\n",
    "    - answer: the generated answer to evaluate\n",
    "    {final_answer}                            \n",
    "    Output format (return only valid JSON):\n",
    "    {format_ins}                \n",
    "    \"\"\", input_variables=['latest_question', 'sub_questions', 'final_answer'], partial_variables={'format_ins': evaluation_format.get_format_instructions()})\n",
    "    chain = prompt | evaluator | evaluation_format\n",
    "    result = chain.invoke({'latest_question' : latest_question, \"sub_questions\" : questions, \"final_answer\" : final_answer})\n",
    "    print(\"Evaluation executed✅\")\n",
    "    return {\"evaluation\" : result.evaluation, \"feedback\" : result.feedback}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "7c1af5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation executed✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': 0.0,\n",
       " 'feedback': 'The answer does not provide any relevant information to address the question. It seems to be a placeholder or an error message, indicating that there is no context match. To improve, the answer should provide a clear and concise explanation of why smoking is bad, including its health risks and effects on the body.'}"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = [\"Why is smoking bad?\"], \n",
    "                  sub_questions=[['What health risks are associated with smoking?', 'How does smoking affect the cardiovascular system?','Why does smoking lead to addiction?']], \n",
    "                  final_answer= 'No context match')\n",
    "\n",
    "evaluation(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "182295f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need conditional function for conditional workflow for either to retry or ask for user clarification\n",
    "def evaluation_conditional_edge(state : RAG_state):\n",
    "    if state['retry_count'] >2 or state['final_answer'] == \"No context match\":\n",
    "        return \"conclude\"\n",
    "    elif state['evaluation'] <= 0.5:\n",
    "        print(\"retry count➿: \", state['retry_count'])\n",
    "        return \"refine\"\n",
    "    else:\n",
    "        return \"clarify\"\n",
    "\n",
    "def human_conditional_edge(state: RAG_state):\n",
    "    if state['verify'] == True:\n",
    "        return END\n",
    "    return 'refine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "18ff6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refining the query by focusing on the way the evaluater has guided and human clarification(if required)\n",
    "def refining_query(state: RAG_state):\n",
    "    # increasing the rety count as refining is retry\n",
    "    latest_question = state['latest_question'][-1]\n",
    "    sub_questions = state['sub_questions'][-1]\n",
    "    final_answer = state['final_answer']\n",
    "    feedback = state['feedback']\n",
    "    if state.get('clarification'):\n",
    "        feedback = {\n",
    "            'feedback_during_evaluation' : feedback,\n",
    "            'clarification_from_user' : state['clarification']\n",
    "        }\n",
    "    template = PromptTemplate(template=\"\"\"You are a query refinement agent.\n",
    "    Your task:\n",
    "    - Refine the original main question using the provided feedback.\n",
    "    - Generate 2–3 refined sub-questions optimized for semantic search and document retrieval.\n",
    "    - Improve clarity, specificity, and retrievability.\n",
    "    - Fix issues mentioned in the feedback (e.g., vagueness, missing aspects, incorrect focus).\n",
    "\n",
    "    Inputs:\n",
    "    - original_main_question: {latest_question}\n",
    "    - original_sub_questions: {sub_questions}\n",
    "    - feedback: {feedback}\n",
    "    - final_answer : {final_answer}\n",
    "\n",
    "    Rules:\n",
    "    - Do NOT answer any question.\n",
    "    - Do NOT add explanations.\n",
    "    - Avoid repeating the original questions unless necessary.\n",
    "    - Each sub-question must target a distinct aspect.\n",
    "    - Each sub-question must be factual, concrete, and searchable.\n",
    "    - Keep sub-questions under 20 words.\n",
    "    - Prefer \"what / how / why\" formulations where appropriate.\n",
    "\n",
    "    Output format:\n",
    "    Return ONLY valid JSON in this exact structure: \n",
    "    {format_ins}\n",
    "    \"\"\", input_variables=['latest_question', 'sub_questions', 'feedback', 'final_answer'], partial_variables={'format_ins' : Refine_format.get_format_instructions()})  \n",
    "\n",
    "    chain = template | model | Refine_format\n",
    "    result = chain.invoke({'latest_question' : latest_question, 'sub_questions' : sub_questions, 'feedback' : feedback, 'final_answer': final_answer})\n",
    "    print(\"RefiningQuery executed✅\")\n",
    "    return {'latest_question' : [result.latest_question], 'sub_questions' : [result.sub_questions], 'retry_count' : state['retry_count'] + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "664b0fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RefiningQuery executed✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'latest_question': ['What are the health effects of smoking?'],\n",
       " 'sub_questions': [['What diseases are linked to smoking?',\n",
       "   'How does smoking harm lungs?',\n",
       "   'What toxins are in cigarette smoke?']],\n",
       " 'retry_count': 2}"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = [\"Why is smoking bad?\"], \n",
    "                  sub_questions=[['What health risks are associated with smoking?', 'How does smoking affect the cardiovascular system?','Why does smoking lead to addiction?']], \n",
    "                  final_answer= 'No context match',\n",
    "                  evaluation= 0.0,\n",
    "                  feedback= \"The answer does not provide any relevant information to address the question. It simply states 'No context match' without explaining why or providing any context. To improve, the answer should directly address the question, providing clear and concise information about the health risks associated with smoking.\",\n",
    "                  retry_count=1\n",
    "                  )\n",
    "refining_query(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "51925a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human clarification for the asnwer and the question\n",
    "def human_clarification(state : RAG_state):\n",
    "    decision = interrupt({\n",
    "        \"type\" : \"preview\",\n",
    "        \"reason\" : \"Agent need to know user's clarification\",\n",
    "        \"instruction\" : \"If Approve the answer then Yes, Reject then give feedback where to point more or clarify more\"\n",
    "    })\n",
    "    if decision == \"yes\":\n",
    "        state[\"verify\"] = True\n",
    "    feedback_from_user = decision\n",
    "    print(\"HumanClarification executed✅\")\n",
    "    return {\"clarification\" : feedback_from_user}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "500c4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion(state: RAG_state):\n",
    "    return {'final_answer' : state['final_answer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "0e410729",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(RAG_state)\n",
    "\n",
    "graph.add_node(\"QueryDecomposition\", query_decomposition)\n",
    "graph.add_node(\"Retriever\", retriever)\n",
    "graph.add_node(\"Summarizer\", summarizer)\n",
    "graph.add_node(\"Evaluation\", evaluation)\n",
    "graph.add_node(\"RefiningQuery\", refining_query)\n",
    "graph.add_node(\"HumanClarification\", human_clarification)\n",
    "graph.add_node(\"Conclusion\", conclusion)\n",
    "\n",
    "graph.add_edge(START, \"QueryDecomposition\")\n",
    "graph.add_edge(\"QueryDecomposition\", \"Retriever\")\n",
    "graph.add_edge(\"Retriever\",\"Summarizer\")\n",
    "graph.add_edge(\"Summarizer\", \"Evaluation\")\n",
    "graph.add_conditional_edges(\"Evaluation\", evaluation_conditional_edge, {'refine' : \"RefiningQuery\", 'clarify' : \"HumanClarification\", \"conclude\" : \"Conclusion\"})\n",
    "graph.add_conditional_edges(\"HumanClarification\", human_conditional_edge, {'accept' : 'Conclusion', 'reject' : \"RefiningQuery\"})\n",
    "graph.add_edge(\"RefiningQuery\", \"QueryDecomposition\")\n",
    "graph.add_edge(\"Conclusion\", END)\n",
    "\n",
    "\n",
    "workflow = graph.compile()\n",
    "# workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "190410e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Decomposition executed✅\n",
      "Retriever executed✅\n",
      "Summarizer executed ✅\n",
      "Evaluation executed✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'latest_question': ['Why is smoking bad?'],\n",
       " 'sub_questions': [['What harms smoking causes',\n",
       "   'How smoking affects health',\n",
       "   'Why quit smoking']],\n",
       " 'answers': [[['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
       "    'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.'],\n",
       "   ['attention and the parameter-free position representation and became the other person involved in nearly every',\n",
       "    'heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training'],\n",
       "   ['versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered',\n",
       "    'heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training'],\n",
       "   ['versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered',\n",
       "    'heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training']]],\n",
       " 'final_answer': 'No context match',\n",
       " 'retry_count': 0,\n",
       " 'evaluation': 0.0,\n",
       " 'feedback': 'The answer does not provide any relevant information to address the question. A good answer should explain the harmful effects of smoking on health, such as lung cancer, heart disease, and respiratory problems. Suggestions for improvement include providing specific examples of health risks associated with smoking and offering alternatives or resources for quitting.'}"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.invoke({\"latest_question\" : [\"Why is smoking bad?\"], 'retry_count' : 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
