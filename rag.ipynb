{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb06bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated, Literal, Optional, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acc39269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the models\n",
    "load_dotenv()\n",
    "model = ChatGroq(\n",
    "    model=os.getenv(\"model\"),\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")\n",
    "\n",
    "evaluator = ChatGroq(\n",
    "    model=os.getenv(\"evaluater_model\"),\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f9875a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path=\"Attention.pdf\")\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7196cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 80)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5682118",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    embedding=embedding_model,\n",
    "    documents=chunks,\n",
    "    collection_name= \"my_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e21371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I am using mmr so that each chunk retrieved covers different angles\n",
    "retrievers = vector_store.as_retriever(search_type = \"mmr\", search_kwargs = {'k' : 2, 'fech_k' : 5, 'lambda_mult' : 0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e841bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Format(BaseModel):\n",
    "    output :list[str]\n",
    "\n",
    "class summary(BaseModel):\n",
    "    summary : str\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    evaluation : float = Field(ge=0, le=1, description=\"this containt the value got for the answer providd to the llm after evaluation.\")\n",
    "    feedback : str = Field(description = \"this should containt the place where the refining should be done, it is like feedback to imporive the evaluation\")\n",
    "\n",
    "op_format = PydanticOutputParser(pydantic_object=Format)\n",
    "summary_format = PydanticOutputParser(pydantic_object=summary)\n",
    "evaluation_format = PydanticOutputParser(pydantic_object=Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13c6e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_state(TypedDict):\n",
    "    latest_question : Annotated[list[HumanMessage], add_messages]\n",
    "    sub_questions : Annotated[list[str], add_messages]\n",
    "    answers : Annotated[list[str], add_messages]\n",
    "    final_answer : str\n",
    "    retry_count : int\n",
    "    evaluation : float = Field(ge=0, le=1)\n",
    "    feedback : str\n",
    "    clarification : str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84b47654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need sqlite db to store the checkpoints to make my agent remember the past topics\n",
    "conn = sqlite3.connect(database='RAG.db', check_same_thread=False)\n",
    "# this creates the checkpointer that saves the checkpoint in the database and allows persistance\n",
    "checkpointer = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc536c",
   "metadata": {},
   "source": [
    "### Query Decomposer:\n",
    "- Query decomposer is used to avoid getting only the content that matches to the question and focusing on getting all the context from the document from all angles. \n",
    "\n",
    " for example:\n",
    "\n",
    " If the question is **Why shouldn't we smoke?**, then the decomposed questions can be\n",
    " * Why somking is bad?\n",
    " * Somking effect on our health\n",
    " * How does it effect our body?\n",
    "\n",
    "are the questions that can hit different angle, which produces the quality answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbc1a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all node's task\n",
    "def query_decomposition(state : RAG_state):\n",
    "    # first when we get the prompt, first we decompose the main question so that we can make multiple diferent questions in the same sense to extract the relevant contnet from the RAG\n",
    "    template = PromptTemplate(template=\"\"\"You are a query decomposition agent.\n",
    "    Your task is to transform a single high-level user question into 2–3 concise, non-overlapping sub-questions optimized for semantic search and retrieval.\n",
    "    Rules:\n",
    "    - Each sub-question must target a distinct angle of the original question.\n",
    "    - Sub-questions must be factual, concrete, and searchable.\n",
    "    - Avoid rephrasing the same question multiple times.\n",
    "    - Do NOT answer the questions.\n",
    "    - Do NOT add explanations.\n",
    "    - Keep each sub-question under 15 words only.\n",
    "    - if the question is simple then make the sub-questions simple too.\n",
    "    - Prefer \"what / how / why\" formulations when useful.\n",
    "    Question: \n",
    "    {question}\n",
    "    Output format:\n",
    "    {fm}\"\"\", input_variables=['question'],partial_variables={'fm' : op_format.get_format_instructions()})\n",
    "    chain = template | model | op_format\n",
    "    output = chain.invoke({\"question\" : state['latest_question']}).output\n",
    "    return {'sub_question' : [output]}\n",
    "\n",
    "\n",
    "# retreiving the relevant content from the docs\n",
    "def retriever(state : RAG_state):\n",
    "    answers = []\n",
    "    answer = retrievers.invoke(state['latest_question'])\n",
    "    answers.append(answer)\n",
    "    for question in state['sub_questions']:\n",
    "        answer = retrievers.invoke(question)\n",
    "        answers.append(answer)\n",
    "    page_content = []\n",
    "    for ans in answers:\n",
    "        element = []\n",
    "        for i in ans:\n",
    "            element.append(i.page_content)\n",
    "        page_content.append(element)\n",
    "    \n",
    "    return {'answers' : page_content}\n",
    "\n",
    "\n",
    "# summarize the answers that we got from each sub-questions in such a way that it provides a different angle for summarizing for each list of sub-question\n",
    "def summarizer(state : RAG_state):\n",
    "    answers = state['answers']\n",
    "    question = state['latest_question']\n",
    "    sub_quesitons = state['sub_questions']\n",
    "    ques_ans_dict : dict[str, list[str]] = {}\n",
    "    ques_ans_dict[question] = answers[0]\n",
    "    for index, answer in enumerate(answers):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        ques_ans_dict[sub_quesitons[index-1]] = answer\n",
    "    template = PromptTemplate(template=\"\"\"You are a summarization agent.\n",
    "    Task:\n",
    "    - Summarize the retrieved chunks for each sub-question.\n",
    "    - Each sub-question maps to a list of concise summary sentences.\n",
    "    - Summaries may mix information from multiple chunks, but must reflect the original content accurately.\n",
    "    - Do not hallucinate; use only the content from the chunks.\n",
    "    - Do not dilute important details.       \n",
    "    - If there is no context in the answer list based on the question then just say \"No context match\". \n",
    "                                                                                                              \n",
    "    Input:\n",
    "    - questions_and_chunks: a JSON object where each key is a sub-question, and each value is a list of strings (chunks retrieved for that sub-question).\\n\n",
    "    {ques_ans_dict}\n",
    "                                                \n",
    "    Output format:\n",
    "    The output should only contain the answer(no question with it)\n",
    "    {format_ins}\n",
    "                        \n",
    "    Rules:\n",
    "    - No explanations outside JSON.\n",
    "    - Preserve the list of strings per sub-question.\n",
    "    - If a sub-question has no content, return [\"Not found in the provided context\"].\n",
    "    \"\"\", input_variables = ['ques_ans_dict'], partial_variables={'format_ins' : summary_format.get_format_instructions()})\n",
    "\n",
    "    chain = template | model | summary_format\n",
    "    result = chain.invoke({\"ques_ans_dict\" : ques_ans_dict}).summary\n",
    "    return {'final_answer' : result}\n",
    "\n",
    "\n",
    "# evaluating the asnwer based on the question and sub_questions generated, if the evaluation is > 0.7 then pass and if still better can be done then retry after refining the query and clarification\n",
    "def evaluation(state : RAG_state):\n",
    "    answer = state['final_answer']\n",
    "    prompt = PromptTemplate(template=\"\"\"You are an evaluation agent.\n",
    "    Task:\n",
    "    - Evaluate the quality of the provided answer to the given question.\n",
    "    - Provide a **score between 0 and 1**, where 1 is perfect and 0 is completely wrong.\n",
    "    - Provide **feedback** explaining why the score was given and suggestions to improve the answer (e.g., missing information, inaccurate, vague, unclear wording).\n",
    "    - Be concise but informative in the feedback.\n",
    "    - Base your evaluation **strictly on the question and the provided answer**. Do not assume extra knowledge.\n",
    "    - Do NOT include the question in the output, only the score and feedback.\n",
    "\n",
    "    Input:\n",
    "    - question: the original user question\n",
    "    - answer: the generated answer to evaluate\n",
    "\n",
    "    Output format (return only valid JSON):\n",
    "    {\n",
    "    \"score\": float,         # between 0 and 1\n",
    "    \"feedback\": string      # concise feedback pointing out issues or improvements\n",
    "    }\n",
    "    {}                \n",
    "    \"\"\", input_variables=[], partial_variables={})\n",
    "    chain = prompt | evaluator | evaluation_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8127d3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_question': [['What health risks are associated with smoking?',\n",
       "   'How does smoking affect the cardiovascular system?',\n",
       "   'Why does smoking increase cancer risk?']]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = \"Why is smoking bad?\")\n",
    "query_decomposition(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b45e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': [['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
       "   'heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training'],\n",
       "  ['the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.',\n",
       "   'were chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].'],\n",
       "  ['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
       "   'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture'],\n",
       "  ['the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.',\n",
       "   'tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.']]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = \"Why is smoking bad?\", sub_questions=['What health risks are associated with smoking?',\n",
    "   'How does smoking affect the cardiovascular system?',\n",
    "   'Why does smoking lead to addiction?'])\n",
    "retriever(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_answer': 'Why is smoking bad? Not found in the provided context. What health risks are associated with smoking? Not found in the provided context. How does smoking affect the cardiovascular system? Not found in the provided context. Why does smoking lead to addiction? Not found in the provided context.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = RAG_state(latest_question = \"Why is smoking bad?\", \n",
    "                  sub_questions=['What health risks are associated with smoking?', 'How does smoking affect the cardiovascular system?','Why does smoking lead to addiction?'], \n",
    "                  answers=[['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
    "                            'heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training'],\n",
    "                            ['the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.',\n",
    "                            'were chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].'],\n",
    "                            ['keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
    "                            'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture'],\n",
    "                            ['the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.',\n",
    "                            'nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.']])\n",
    "summarizer(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e410729",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m graph.add_node(\u001b[33m\"\u001b[39m\u001b[33mRetriever\u001b[39m\u001b[33m\"\u001b[39m, retriever)\n\u001b[32m      5\u001b[39m graph.add_node(\u001b[33m\"\u001b[39m\u001b[33mSummarizer\u001b[39m\u001b[33m\"\u001b[39m, summarizer)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m graph.add_node(\u001b[33m\"\u001b[39m\u001b[33mEvaluation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mevaluation\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "graph = StateGraph(RAG_state)\n",
    "\n",
    "graph.add_node(\"query_decomposition\", query_decomposition)\n",
    "graph.add_node(\"Retriever\", retriever)\n",
    "graph.add_node(\"Summarizer\", summarizer)\n",
    "graph.add_node(\"Evaluation\", evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
