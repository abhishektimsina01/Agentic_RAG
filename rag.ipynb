{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\_internal\\_fields.py:198: UserWarning: Field name \"output_schema\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nitro\\AppData\\Roaming\\Python\\Python313\\site-packages\\pydantic\\_internal\\_fields.py:198: UserWarning: Field name \"stream\" in \"TavilyResearch\" shadows an attribute in parent \"BaseTool\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated, Literal, Optional, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc39269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the models\n",
    "load_dotenv()\n",
    "model = ChatGroq(\n",
    "    model=os.getenv(\"model\"),\n",
    "    api_key=os.getenv(\"api_key\")\n",
    ")\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9875a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path=\"Attention.pdf\")\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7196cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 80)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5682118",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    embedding=embedding_model,\n",
    "    documents=chunks,\n",
    "    collection_name= \"my_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e21371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievers = vector_store.as_retriever(search_type = \"mmr\", search_kwargs = {'k' : 3, 'lambda_mult' : 0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e841bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Format(BaseModel):\n",
    "    output :list[str]\n",
    "\n",
    "op_format = PydanticOutputParser(pydantic_object=Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13c6e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_state(BaseModel):\n",
    "    latest_question : Annotated[list[HumanMessage], add_messages]\n",
    "    sub_questions : Annotated[list[str], add_messages]\n",
    "    answers : Annotated[list[str], add_messages]\n",
    "    final_answer : str\n",
    "    retry_count : int\n",
    "    evaluation : float = Field(ge=0, le=1)\n",
    "    clarification : str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84b47654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I need sqlite db to store the checkpoints to make my agent remember the past topics\n",
    "# conn = sqlite3.connect(database='RAG.db', check_same_thread=False)\n",
    "# # this creates the checkpointer that saves the checkpoint in the database and allows persistance\n",
    "# checkpointer = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all node's task\n",
    "def query_decomposition(state : RAG_state):\n",
    "    # first when we get the prompt, first we decompose the main question so that we can make multiple diferent questions in the same sense to extract the relevant contnet from the RAG\n",
    "    template = PromptTemplate(template=\"\"\"You are a query decomposition agent.\n",
    "    Your task is to transform a single high-level user question into 2â€“3 concise, non-overlapping sub-questions optimized for semantic search and retrieval.\n",
    "    Rules:\n",
    "    - Each sub-question must target a distinct angle of the original question.\n",
    "    - Sub-questions must be factual, concrete, and searchable.\n",
    "    - Avoid rephrasing the same question multiple times.\n",
    "    - Do NOT answer the questions.\n",
    "    - Do NOT add explanations.\n",
    "    - Keep each sub-question under 15 words only.\n",
    "    - Prefer \"what / how / why\" formulations when useful.\n",
    "    Question: \n",
    "    {question}\n",
    "    Output format:\n",
    "    {fm}\"\"\", input_variables=['question'],partial_variables={'fm' : op_format.get_format_instructions()})\n",
    "    chain = template | model | op_format\n",
    "    output = chain.invoke({\"question\" : state['latest_question']}).output\n",
    "    return {'sub_question' : [output]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e410729",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(RAG_state)\n",
    "\n",
    "graph.add_node(\"query_decomposition\", query_decomposition)\n",
    "graph.add_node(\"Retriever\", retriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
